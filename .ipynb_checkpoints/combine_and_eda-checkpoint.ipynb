{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "my_zip = zipfile.ZipFile('RawData.zip')\n",
    "my_zip.extractall(os.getcwd())\n",
    "my_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 6 daily_44201.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\daily_44201_2012.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\daily_44201_2013.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\daily_44201_2014.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\daily_44201_2015.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\daily_44201_2016.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\daily_44201_2017.csv\n",
      "'Summarized_daily_ozone.csv' generated!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "allFiles=str(os.getcwd())+\"\\daily_44201_*.csv\"\n",
    "fileName = \"Summarized_daily_ozone.csv\"\n",
    "filepath = Path(fileName)\n",
    "filelist = glob.glob(allFiles)  \n",
    "heading = 0\n",
    "if(filelist) != 0:\n",
    "    print(\"Total %d daily_44201.csv\" %len(filelist))\n",
    "    with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "        for f in filelist:                                                                                                                    \n",
    "            print(\"Processing \" + f)\n",
    "            df = pd.read_csv(f, skipinitialspace=True)\n",
    "            if heading == 0:\n",
    "                df.columns = ['State_Code','County_Code','Site_Num','Parameter_Code','POC','Latitude','Longitude','Datum','Parameter_Name','Sample_Duration','Pollutant_Standard','Date_Local','Units_of_Measure','Event_Type' ,'Observation_Count','Observation_Percent','Arithmetic_Mean','1st_Max_Value','1st_Max_Hour','AQI','Method_Code','Method_Name','Local_Site_Name','Address','State_Name','County_Name','City_Name','CBSA_Name','Date_of_Last_Change']\n",
    "                df.to_csv(file, header=True,index=False)\n",
    "                heading = 1 \n",
    "            else:\n",
    "                df.to_csv(file, header=False, index=False, mode='a')\n",
    "        print(\"'Summarized_daily_ozone.csv' generated!\" )\n",
    "else:\n",
    "    print(\"file list is empty!!\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 6 daily_42401.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\SO2daily\\daily_42401_2012.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\SO2daily\\daily_42401_2013.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\SO2daily\\daily_42401_2014.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\SO2daily\\daily_42401_2015.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\SO2daily\\daily_42401_2016.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\SO2daily\\daily_42401_2017.csv\n",
      "'Summarized_daily_SO2.csv' generated!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data')\n",
    "so2Files=str(os.getcwd())+\"\\SO2daily\\daily_42401_*.csv\"\n",
    "fileName = \"Summarized_daily_SO2.csv\"\n",
    "filepath = Path(fileName)\n",
    "so2Fileslist = glob.glob(so2Files)  \n",
    "heading = 0\n",
    "if(so2Fileslist) != 0:\n",
    "    print(\"Total %d daily_42401.csv\" %len(so2Fileslist))\n",
    "    with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "        for f in so2Fileslist:                                                                                                                    \n",
    "            print(\"Processing \" + f)\n",
    "            df = pd.read_csv(f, skipinitialspace=True)\n",
    "            if heading == 0:\n",
    "                df.columns = ['State_Code','County_Code','Site_Num','Parameter_Code','POC','Latitude','Longitude','Datum','Parameter_Name','Sample_Duration','Pollutant_Standard','Date_Local','Units_of_Measure','Event_Type' ,'Observation_Count','Observation_Percent','Arithmetic_Mean','Max_Value','1st_Max_Hour','AQI','Method_Code','Method_Name','Local_Site_Name','Address','State_Name','County_Name','City_Name','CBSA_Name','Date_of_Last_Change']\n",
    "                df.to_csv(file, header=True,index=False)\n",
    "                heading = 1 \n",
    "            else:\n",
    "                df.to_csv(file, header=False, index=False, mode='a')\n",
    "        print(\"'Summarized_daily_SO2.csv' generated!\" )\n",
    "else:\n",
    "    print(\"file list is empty!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-693264ff3121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mno2Files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\NO2daily\\daily_42602_*.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Summarized_daily_NO2.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mno2Fileslist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno2Files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data')\n",
    "no2Files=str(os.getcwd()) + \"\\\\NO2daily\\daily_42602_*.csv\"\n",
    "fileName = \"Summarized_daily_NO2.csv\"\n",
    "filepath = Path(fileName)\n",
    "no2Fileslist = glob.glob(no2Files)  \n",
    "heading = 0\n",
    "if(no2Fileslist) != 0:\n",
    "    print(\"Total %d daily_42602.csv\" %len(no2Fileslist))\n",
    "    with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "        for f in no2Fileslist:                                                                                                                    \n",
    "            print(\"Processing \" + f)\n",
    "            df = pd.read_csv(f, skipinitialspace=True)\n",
    "            if heading == 0:\n",
    "                df.columns = ['State_Code','County_Code','Site_Num','Parameter_Code','POC','Latitude','Longitude','Datum','Parameter_Name','Sample_Duration','Pollutant_Standard','Date_Local','Units_of_Measure','Event_Type' ,'Observation_Count','Observation_Percent','Arithmetic_Mean','1st_Max_Value','1st_Max_Hour','AQI','Method_Code','Method_Name','Local_Site_Name','Address','State_Name','County_Name','City_Name','CBSA_Name','Date_of_Last_Change']\n",
    "                df.to_csv(file, header=True,index=False)\n",
    "                heading = 1 \n",
    "            else:\n",
    "                df.to_csv(file, header=False, index=False, mode='a')\n",
    "        print(\"'Summarized_daily_NO2.csv' generated!\" )\n",
    "else:\n",
    "    print(\"file list is empty!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-43129fb28e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#os.chdir('C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoFiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\COdaily\\daily_42101_*.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Summarized_daily_CO.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcoFileslist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoFiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#os.chdir('C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data')\n",
    "coFiles=str(os.getcwd()) + \"\\\\COdaily\\daily_42101_*.csv\"\n",
    "fileName = \"Summarized_daily_CO.csv\"\n",
    "filepath = Path(fileName)\n",
    "coFileslist = glob.glob(coFiles)  \n",
    "heading = 0\n",
    "if(coFileslist) != 0:\n",
    "    print(\"Total %d daily_42101.csv\" %len(coFileslist))\n",
    "    with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "        for f in coFileslist:                                                                                                                    \n",
    "            print(\"Processing \" + f)\n",
    "            df = pd.read_csv(f, skipinitialspace=True)\n",
    "            if heading == 0:\n",
    "                df.columns = ['State_Code','County_Code','Site_Num','Parameter_Code','POC','Latitude','Longitude','Datum','Parameter_Name','Sample_Duration','Pollutant_Standard','Date_Local','Units_of_Measure','Event_Type' ,'Observation_Count','Observation_Percent','Arithmetic_Mean','1st_Max_Value','1st_Max_Hour','AQI','Method_Code','Method_Name','Local_Site_Name','Address','State_Name','County_Name','City_Name','CBSA_Name','Date_of_Last_Change']\n",
    "                df.to_csv(file, header=True,index=False)\n",
    "                heading = 1 \n",
    "            else:\n",
    "                df.to_csv(file, header=False, index=False, mode='a')\n",
    "        print(\"'Summarized_daily_CO.csv' generated!\" )\n",
    "else:\n",
    "    print(\"file list is empty!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 6 daily_88101_*.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2012.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2013.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2014.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2015.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2016.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2017.csv\n",
      "'Summarized_daily_PM2.5.csv' generated!\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data')\n",
    "PM25Files=str(os.getcwd()) + \"\\\\PM2.5daily\\daily_88101_*.csv\"\n",
    "fileName = \"Summarized_daily_PM2.5.csv\"\n",
    "filepath = Path(fileName)\n",
    "PM25Fileslist = glob.glob(PM25Files)  \n",
    "heading = 0\n",
    "if(PM25Fileslist) != 0:\n",
    "    print(\"Total %d daily_88101_*.csv\" %len(PM25Fileslist))\n",
    "    with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "        for f in PM25Fileslist:                                                                                                                    \n",
    "            print(\"Processing \" + f)\n",
    "            df = pd.read_csv(f, skipinitialspace=True)\n",
    "            if heading == 0:\n",
    "                df.columns = ['State_Code','County_Code','Site_Num','Parameter_Code','POC','Latitude','Longitude','Datum','Parameter_Name','Sample_Duration','Pollutant_Standard','Date_Local','Units_of_Measure','Event_Type' ,'Observation_Count','Observation_Percent','Arithmetic_Mean','Max_Value','1st_Max_Hour','AQI','Method_Code','Method_Name','Local_Site_Name','Address','State_Name','County_Name','City_Name','CBSA_Name','Date_of_Last_Change']\n",
    "                df.to_csv(file, header=True,index=False)\n",
    "                heading = 1 \n",
    "            else:\n",
    "                df.to_csv(file, header=False, index=False, mode='a')\n",
    "        print(\"'Summarized_daily_PM2.5.csv' generated!\" )\n",
    "else:\n",
    "    print(\"file list is empty!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 6 daily_88101_*.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2012.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2013.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2014.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2015.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2016.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM2.5daily\\daily_88101_2017.csv\n",
      "'Summarized_daily_PM2.5.csv' generated!\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data')\n",
    "PM25Files=str(os.getcwd()) + \"\\\\PM2.5daily\\daily_88101_*.csv\"\n",
    "fileName = \"Summarized_daily_PM2.5.csv\"\n",
    "filepath = Path(fileName)\n",
    "PM25Fileslist = glob.glob(PM25Files)  \n",
    "heading = 0\n",
    "if(PM25Fileslist) != 0:\n",
    "    print(\"Total %d daily_88101_*.csv\" %len(PM25Fileslist))\n",
    "    with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "        for f in PM25Fileslist:                                                                                                                    \n",
    "            print(\"Processing \" + f)\n",
    "            df = pd.read_csv(f, skipinitialspace=True)\n",
    "            if heading == 0:\n",
    "                df.columns = ['State_Code','County_Code','Site_Num','Parameter_Code','POC','Latitude','Longitude','Datum','Parameter_Name','Sample_Duration','Pollutant_Standard','Date_Local','Units_of_Measure','Event_Type' ,'Observation_Count','Observation_Percent','Arithmetic_Mean','Max_Value','1st_Max_Hour','AQI','Method_Code','Method_Name','Local_Site_Name','Address','State_Name','County_Name','City_Name','CBSA_Name','Date_of_Last_Change']\n",
    "                df.to_csv(file, header=True,index=False)\n",
    "                heading = 1 \n",
    "            else:\n",
    "                df.to_csv(file, header=False, index=False, mode='a')\n",
    "        print(\"'Summarized_daily_PM2.5.csv' generated!\" )\n",
    "else:\n",
    "    print(\"file list is empty!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Summarized_daily_PM10.csv\",low_memory=False)\n",
    "df1['Site_Num'] = df1['Site_Num'].astype('str').str.zfill(4)\n",
    "df1['County_Code'] = df1['County_Code'].astype('str').str.zfill(3)\n",
    "df1['State_Code'] = df1.State_Code.astype('str').str.cat(df1.County_Code).str.cat(df1.Site_Num)\n",
    "df1.drop(df1.columns[[1, 2]], axis=1,inplace=True)\n",
    "df1[['Date_Local']] = df1[['Date_Local']].astype('str')\n",
    "df1['Date_Local'] = df1['Date_Local'].str.replace('-','')\n",
    "df1[['Date_Local']] = df1[['Date_Local']].astype('int64')\n",
    "fileName = \"Summarized_daily_PM10.csv\"\n",
    "with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "    df1.to_csv(file, header=True,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 6 daily_81102_*.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM10daily\\daily_81102_2012.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM10daily\\daily_81102_2013.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM10daily\\daily_81102_2014.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM10daily\\daily_81102_2015.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM10daily\\daily_81102_2016.csv\n",
      "Processing C:\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data\\PM10daily\\daily_81102_2017.csv\n",
      "'Summarized_daily_PM10.csv' generated!\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\Snigdha\\Documents\\ADS\\Final Project\\daily_individual_data')\n",
    "PM10Files= str(os.getcwd()) +\"\\PM10daily\\daily_81102_*.csv\"\n",
    "fileName = \"Summarized_daily_PM10.csv\"\n",
    "filepath = Path(fileName)\n",
    "PM10Fileslist = glob.glob(PM10Files)  \n",
    "heading = 0\n",
    "if(PM10Fileslist) != 0:\n",
    "    print(\"Total %d daily_81102_*.csv\" %len(PM10Fileslist))\n",
    "    with open(fileName, 'w',encoding='utf-8', newline=\"\") as file:\n",
    "        for f in PM10Fileslist:                                                                                                                    \n",
    "            print(\"Processing \" + f)\n",
    "            df = pd.read_csv(f, skipinitialspace=True)\n",
    "            if heading == 0:\n",
    "                df.columns = ['State_Code','County_Code','Site_Num','Parameter_Code','POC','Latitude','Longitude','Datum','Parameter_Name','Sample_Duration','Pollutant_Standard','Date_Local','Units_of_Measure','Event_Type' ,'Observation_Count','Observation_Percent','Arithmetic_Mean','Max_Value','1st_Max_Hour','AQI','Method_Code','Method_Name','Local_Site_Name','Address','State_Name','County_Name','City_Name','CBSA_Name','Date_of_Last_Change']\n",
    "                df.to_csv(file, header=True,index=False)\n",
    "                heading = 1 \n",
    "            else:\n",
    "                df.to_csv(file, header=False, index=False, mode='a')\n",
    "        print(\"'Summarized_daily_PM10.csv' generated!\" )\n",
    "else:\n",
    "    print(\"file list is empty!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State_Code                  0\n",
       "Parameter_Code              0\n",
       "POC                         0\n",
       "Latitude                    0\n",
       "Longitude                   0\n",
       "Datum                       0\n",
       "Parameter_Name              0\n",
       "Sample_Duration             0\n",
       "Pollutant_Standard          0\n",
       "Date_Local                  0\n",
       "Units_of_Measure            0\n",
       "Event_Type                  0\n",
       "Observation_Count           0\n",
       "Observation_Percent         0\n",
       "Arithmetic_Mean             0\n",
       "1st_Max_Value               0\n",
       "1st_Max_Hour                0\n",
       "AQI                         0\n",
       "Method_Name                 0\n",
       "Local_Site_Name        104627\n",
       "Address                     0\n",
       "State_Name                  0\n",
       "County_Name                 0\n",
       "City_Name                   0\n",
       "CBSA_Name              207187\n",
       "Date_of_Last_Change         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 =  pd.read_csv(\"Summarized_daily_ozone.csv\",low_memory=False)\n",
    "df1 = df1.drop('Method_Code',axis=1)\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Linear Regression---------------\n",
      "Train Data:\n",
      "Mean Absolute Error:  4.38988828139\n",
      "Root of Mean Squared Deviation:  6.745477074883096\n",
      "None\n",
      "Test Data:\n",
      "Mean Absolute Error:  4.38801552929\n",
      "Root of Mean Squared Deviation:  6.734475024312339\n",
      "None\n",
      "---------------Random Forest---------------\n",
      "Train Data:\n",
      "Mean Absolute Error:  0.00604447918434\n",
      "Root of Mean Squared Deviation:  0.3431471448000984\n",
      "None\n",
      "Test Data:\n",
      "Mean Absolute Error:  0.00983101470187\n",
      "Root of Mean Squared Deviation:  0.6193625859253024\n",
      "None\n",
      "---------------Random Forest---------------\n",
      "Train Data:\n",
      "Mean Absolute Error:  0.280208257634\n",
      "Root of Mean Squared Deviation:  0.7601540046003697\n",
      "None\n",
      "Test Data:\n",
      "Mean Absolute Error:  0.279682262218\n",
      "Root of Mean Squared Deviation:  0.7310861555278891\n",
      "None\n",
      "Training score is  0.998338120965\n",
      "Testing score is  0.998459721081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import *\n",
    "lr=linear_model.LinearRegression()\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#os.chdir('ozonedaily')\n",
    "df1 =  pd.read_csv(\"Summarized_daily_ozone.csv\",low_memory=False)\n",
    "df1 = df1.drop('Method_Code',axis=1)\n",
    "train, test = train_test_split(df1, test_size = 0.2)\n",
    "\n",
    "def calculatestat(lr,xaxis,yaxis):\n",
    "    y_pred=lr.predict(xaxis)\n",
    "    MAE=mean_absolute_error(yaxis,y_pred)\n",
    "    print(\"Mean Absolute Error: \", MAE) \n",
    "    RMSE=math.sqrt(mean_squared_error(yaxis,y_pred))\n",
    "    print(\"Root of Mean Squared Deviation: \",RMSE)\n",
    "    \n",
    "def LinearRegressionAnalysis(xtrain,ytrain,xtest,ytest):\n",
    "    trainreg  = lr.fit(xtrain, ytrain)\n",
    "    print(\"---------------Linear Regression---------------\")\n",
    "    print(\"Train Data:\")\n",
    "    print(calculatestat(trainreg, xtrain, ytrain ))\n",
    "    print(\"Test Data:\")\n",
    "    print(calculatestat(trainreg, xtest, ytest))\n",
    "    \n",
    "def Random_Forest(xtrain,ytrain,xtest,ytest):\n",
    "    rand_forest = RandomForestRegressor(n_estimators=15,max_depth=10)\n",
    "    rand_forest = rand_forest.fit(xtrain,ytrain)\n",
    "    print(\"---------------Random Forest---------------\")\n",
    "    print(\"Train Data:\")\n",
    "    print(calculatestat(rand_forest, xtrain, ytrain ))\n",
    "    print(\"Test Data:\")\n",
    "    print(calculatestat(rand_forest,xtest,ytest))\n",
    "    \n",
    "def Neural_Network(xtrain,ytrain,xtest,ytest):\n",
    "    #neural network\n",
    "    xtrain = StandardScaler().fit_transform(xtrain)\n",
    "    xtest = StandardScaler().fit_transform(xtest)\n",
    "    mlp = MLPRegressor( hidden_layer_sizes=20,max_iter = 200, shuffle=True, random_state=1)\n",
    "    fit = mlp.fit(xtrain, ytrain)\n",
    "    print(\"---------------Neural Forest---------------\")\n",
    "    print(\"Train Data:\")\n",
    "    print(calculatestat(fit, xtrain, ytrain ))\n",
    "    print(\"Test Data:\")\n",
    "    print(calculatestat(fit,xtest,ytest))\n",
    "    print(\"Training score is \",mlp.score(xtrain, ytrain))\n",
    "    print(\"Testing score is \",mlp.score(xtest, ytest))\n",
    "\n",
    "    \n",
    "ytrain = train.AQI\n",
    "xtrain = train.drop('AQI',axis=1)._get_numeric_data()\n",
    "ytest = test.AQI\n",
    "xtest = test.drop('AQI',axis=1)._get_numeric_data()\n",
    "LinearRegressionAnalysis(xtrain,ytrain,xtest,ytest)\n",
    "Random_Forest(xtrain,ytrain,xtest,ytest)\n",
    "Neural_Network(xtrain,ytrain,xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE: \n",
      "RFE Features sorted by their rank:  0.868466288479\n",
      "[(5, 'State_Code'), (1, 'POC'), (3, 'Latitude'), (2, 'Longitude'), (4, 'Date_Local'), (1, 'Observation_Count'), (1, 'Observation_Percent'), (1, 'Arithmetic_Mean'), (1, '1st_Max_Value'), (1, '1st_Max_Hour')]\n",
      "--------------------------------------------------\n",
      "K best:\n",
      "K best score: 0.868638528201\n",
      "[('State_Code', 111.5073414900608), ('POC', 20.382604469602214), ('Latitude', 28.457504351906994), ('Longitude', 214.35450411712358), ('Date_Local', 123.87154278297174), ('Observation_Count', 119.68546917754333), ('Observation_Percent', 122.16613516073123), ('Arithmetic_Mean', 52604.993448261586), ('1st_Max_Value', 6683471.7552595995), ('1st_Max_Hour', 29.792727587398705)]\n",
      "--------------------------------------------------\n",
      "Randomized Lasso:\n",
      "Features sorted by their score:\n",
      "[(1.0, 'State_Code'), (1.0, 'POC'), (1.0, 'Latitude'), (1.0, 'Longitude'), (1.0, 'Date_Local'), (1.0, 'Observation_Count'), (1.0, 'Observation_Percent'), (1.0, 'Arithmetic_Mean'), (1.0, '1st_Max_Value'), (1.0, '1st_Max_Hour')]\n",
      "--------------------------------------------------\n",
      "Percentile Selection:\n",
      "Features sorted by score:\n",
      "[(11479.59, 'State_Code'), (1261.1400000000001, 'POC'), (282.74000000000001, 'Latitude'), (23157.5, 'Longitude'), (9657.6800000000003, 'Date_Local'), (3891.9899999999998, 'Observation_Count'), (5108.6300000000001, 'Observation_Percent'), (3130929.8599999999, 'Arithmetic_Mean'), (10074981.869999999, '1st_Max_Value'), (490.00999999999999, '1st_Max_Hour')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "#os.chdir('ozonedaily')\n",
    "df1 =  pd.read_csv(\"Summarized_daily_ozone.csv\",low_memory=False)\n",
    "df1 = df1.drop(['Method_Code','CBSA_Name','Local_Site_Name','Parameter_Code'],axis=1)\n",
    "\n",
    "train, test = train_test_split(df1, test_size = 0.2)\n",
    "ytrain = train.AQI\n",
    "xtrain = train.drop('AQI',axis=1)._get_numeric_data()\n",
    "ytest = test.AQI\n",
    "xtest = test.drop('AQI',axis=1)._get_numeric_data()\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "rfe = RFE(lr, n_features_to_select=6,step=1)\n",
    "rfe = rfe.fit(xtrain,ytrain)\n",
    "s1 = r2_score(ytrain,rfe.predict(xtrain))\n",
    "print (\"RFE: \")\n",
    "print (\"RFE Features sorted by their rank: \",s1)\n",
    "#print (list(zip(xtrain.columns,rfe.ranking_)))\n",
    "print(list(zip(map(lambda x: round(x, 2), rfe.ranking_),xtrain )))\n",
    "print (\"--------------------------------------------------\")\n",
    "\n",
    "print (\"K best:\")\n",
    "feat=SelectKBest(score_func = f_classif ,k=6) \n",
    "feat = feat.fit(xtrain,ytrain)\n",
    "x_train = feat.fit_transform(xtrain,ytrain)\n",
    "kbest = lr.fit(x_train,ytrain)\n",
    "score = lr.score(x_train, ytrain)\n",
    "sc=r2_score(ytrain,lr.predict(x_train))\n",
    "print(\"K best score:\", sc)\n",
    "print(list(zip(xtrain,K.scores_)))\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "rlasso = RandomizedLasso(alpha= 'bic',verbose=False, n_resampling=100, n_jobs=1)\n",
    "rlasso.fit(xtrain,ytrain)\n",
    "print(\"Randomized Lasso:\")\n",
    "print(\"Features sorted by their score:\")\n",
    "coef = pd.DataFrame(rlasso.scores_, columns = ['RandomizedLasso_score'])\n",
    "print (list(zip(map(lambda x: round(x, 2), rlasso.scores_), xtrain.columns.values)))\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "print(\"Percentile Selection:\")\n",
    "fs = SelectPercentile(score_func = f_regression, percentile=20)\n",
    "fs = fs.fit(xtrain,ytrain)\n",
    "print('Features sorted by score:')\n",
    "print(list(zip(map(lambda x: round(x, 2), fs.scores_), xtrain.columns.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
